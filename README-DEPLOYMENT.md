# üç∑ Les Caves d'Albert - Guide de D√©ploiement Complet

## üìã Architecture du Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kafka Producer ‚îÇ (Docker)
‚îÇ   Wine Shop     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Kafka Topics
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kafka Consumer ‚îÇ (Docker)
‚îÇ   ‚Üí Snowflake   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SNOWFLAKE - CAVES_ALBERT_DB                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ  üì• RAW LAYER (Bronze)                                  ‚îÇ
‚îÇ  ‚îî‚îÄ RAW_DATA.RAW_EVENTS_STREAM                         ‚îÇ
‚îÇ     ‚îú‚îÄ EVENT_TYPE (VARCHAR)                            ‚îÇ
‚îÇ     ‚îú‚îÄ PRODUCT_ID (INTEGER)                            ‚îÇ
‚îÇ     ‚îú‚îÄ CUSTOMER_ID (INTEGER)                           ‚îÇ
‚îÇ     ‚îú‚îÄ EVENT_METADATA (OBJECT)                         ‚îÇ
‚îÇ     ‚îî‚îÄ EVENT_CONTENT (VARIANT - JSON complet)          ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº STREAM_RAW_EVENTS (CDC)                      ‚îÇ
‚îÇ         ‚ñº TASK_RAW_TO_STAGING_DISTRIBUTOR (1 min)      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  üîÑ STAGING LAYER (Silver)                              ‚îÇ
‚îÇ  ‚îú‚îÄ STAGING.STG_ORDERS                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Commandes transform√©es                          ‚îÇ
‚îÇ  ‚îî‚îÄ STAGING.STG_INVENTORY_ADJUSTMENTS                  ‚îÇ
‚îÇ     ‚îî‚îÄ Ajustements d'inventaire transform√©s            ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ         ‚ñº STREAM_STG_ORDERS (CDC)                      ‚îÇ
‚îÇ         ‚ñº STREAM_STG_INVENTORY_FOR_HISTORY (CDC)       ‚îÇ
‚îÇ         ‚ñº STREAM_STG_INVENTORY_FOR_CURRENT (CDC)       ‚îÇ
‚îÇ         ‚ñº 3 TASKS automatis√©es                         ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚ú® PRODUCTION LAYER (Gold)                             ‚îÇ
‚îÇ  ‚îú‚îÄ PRODUCTION.ORDERS                                  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Commandes finales (MERGE - pas de doublons)    ‚îÇ
‚îÇ  ‚îú‚îÄ PRODUCTION.INVENTORY_HISTORY                       ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Historique complet des mouvements              ‚îÇ
‚îÇ  ‚îî‚îÄ PRODUCTION.INVENTORY_CURRENT                       ‚îÇ
‚îÇ     ‚îî‚îÄ √âtat actuel de l'inventaire par produit        ‚îÇ
‚îÇ                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ D√©ploiement - √âtape par √âtape

### **Pr√©requis**

‚úÖ Docker Desktop install√© et en cours d'ex√©cution  
‚úÖ Compte Snowflake actif (CAVES_ALBERT_DB cr√©√©e)  
‚úÖ Warehouse COMPUTE_WH disponible  
‚úÖ Python 3.9+ avec environnement `etl-snowflake`

---

### **1Ô∏è‚É£ D√©marrer l'infrastructure Kafka**

```bash
cd /Users/mory_jr/Projet-Snowflake

# D√©marrer tous les conteneurs Docker
docker-compose up -d

# V√©rifier que tout tourne
docker ps

# Vous devriez voir :
# - redpanda (broker Kafka)
# - producer (g√©n√®re des √©v√©nements wine shop)
# - consumer (ins√®re dans Snowflake)
```

**Logs du consumer (pour v√©rifier)** :
```bash
docker logs consumer --tail 50 --follow
```

Vous devriez voir :
```
‚úÖ Batch committed successfully!
üìà Session stats - Orders: XXXX, Inventory: XXXX, Errors: 0
```

---

### **2Ô∏è‚É£ V√©rifier que RAW_EVENTS_STREAM re√ßoit des donn√©es**

Dans Snowflake, ex√©cutez :

```sql
USE DATABASE CAVES_ALBERT_DB;
USE SCHEMA RAW_DATA;

-- Compter les √©v√©nements
SELECT COUNT(*) AS TOTAL_EVENTS FROM RAW_EVENTS_STREAM;

-- Voir un √©chantillon
SELECT 
    EVENT_TYPE,
    PRODUCT_ID,
    CUSTOMER_ID,
    EVENT_CONTENT,
    INGESTION_TIME
FROM RAW_EVENTS_STREAM
LIMIT 10;

-- Distribution par type d'√©v√©nement
SELECT 
    EVENT_TYPE,
    COUNT(*) AS COUNT
FROM RAW_EVENTS_STREAM
GROUP BY EVENT_TYPE;
```

**R√©sultat attendu** :
```
EVENT_TYPE          | COUNT
--------------------|-------
ORDER_CREATED       | ~70%
INVENTORY_ADJUSTED  | ~30%
```

---

### **3Ô∏è‚É£ D√©ployer le pipeline Snowflake (Tasks & Streams)**

**Ouvrez Snowflake Worksheet** et ex√©cutez **TOUT LE CONTENU** de :
```
snowflake-tasks-streams-CORRECTED.sql
```

Ce script va :
1. ‚úÖ Cr√©er les sch√©mas STAGING et PRODUCTION
2. ‚úÖ Cr√©er 5 tables (2 staging + 3 production)
3. ‚úÖ Cr√©er 4 streams CDC
4. ‚úÖ Cr√©er 4 tasks automatis√©es
5. ‚úÖ Activer les tasks

**Temps d'ex√©cution** : ~30 secondes

---

### **4Ô∏è‚É£ V√©rifier que les tasks sont actives**

```sql
-- V√©rifier l'√©tat des tasks
SHOW TASKS IN DATABASE CAVES_ALBERT_DB;

-- Vous devriez voir 4 tasks avec state = 'started'
```

**Output attendu** :
```
NAME                                    | STATE   | SCHEDULE
----------------------------------------|---------|----------
TASK_RAW_TO_STAGING_DISTRIBUTOR        | started | 1 MINUTE
TASK_STAGING_TO_PROD_ORDERS            | started | (child)
TASK_STAGING_TO_PROD_INVENTORY_HISTORY | started | (child)
TASK_STAGING_TO_PROD_INVENTORY_CURRENT | started | (child)
```

---

### **5Ô∏è‚É£ Attendre 2-3 minutes et v√©rifier la propagation des donn√©es**

```sql
-- Vue d'ensemble compl√®te
SELECT 'RAW' AS LAYER, 'RAW_EVENTS_STREAM' AS TABLE_NAME, COUNT(*) AS ROWS 
FROM RAW_DATA.RAW_EVENTS_STREAM
UNION ALL
SELECT 'STAGING', 'STG_ORDERS', COUNT(*) FROM STAGING.STG_ORDERS
UNION ALL
SELECT 'STAGING', 'STG_INVENTORY_ADJUSTMENTS', COUNT(*) FROM STAGING.STG_INVENTORY_ADJUSTMENTS
UNION ALL
SELECT 'PRODUCTION', 'ORDERS', COUNT(*) FROM PRODUCTION.ORDERS
UNION ALL
SELECT 'PRODUCTION', 'INVENTORY_HISTORY', COUNT(*) FROM PRODUCTION.INVENTORY_HISTORY
UNION ALL
SELECT 'PRODUCTION', 'INVENTORY_CURRENT', COUNT(*) FROM PRODUCTION.INVENTORY_CURRENT
ORDER BY LAYER, TABLE_NAME;
```

**R√©sultat attendu** :
```
LAYER      | TABLE_NAME                  | ROWS
-----------|-----------------------------|---------
RAW        | RAW_EVENTS_STREAM           | ~10,000+
STAGING    | STG_ORDERS                  | ~7,000+
STAGING    | STG_INVENTORY_ADJUSTMENTS   | ~3,000+
PRODUCTION | ORDERS                      | ~7,000+
PRODUCTION | INVENTORY_HISTORY           | ~3,000+
PRODUCTION | INVENTORY_CURRENT           | ~500-1000
```

Si les tables STAGING et PRODUCTION sont encore vides :
- Attendre 1-2 minutes de plus (les tasks s'ex√©cutent toutes les 1 minute)
- Ou ex√©cuter manuellement : `EXECUTE TASK TASK_RAW_TO_STAGING_DISTRIBUTOR;`

---

### **6Ô∏è‚É£ Tester les requ√™tes analytiques**

```sql
-- üç∑ Top 10 des vins les plus vendus
SELECT
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    COUNT(*) AS ORDER_COUNT,
    SUM(QUANTITY) AS TOTAL_QUANTITY,
    ROUND(SUM(TOTAL_AMOUNT), 2) AS TOTAL_REVENUE
FROM PRODUCTION.ORDERS
GROUP BY PRODUCT_NAME, PRODUCT_CATEGORY
ORDER BY TOTAL_REVENUE DESC
LIMIT 10;

-- üìä √âtat actuel de l'inventaire par cat√©gorie
SELECT
    PRODUCT_CATEGORY,
    COUNT(DISTINCT PRODUCT_ID) AS PRODUCT_COUNT,
    SUM(CURRENT_STOCK_LEVEL) AS TOTAL_STOCK,
    ROUND(AVG(CURRENT_STOCK_LEVEL), 0) AS AVG_STOCK_PER_PRODUCT
FROM PRODUCTION.INVENTORY_CURRENT
GROUP BY PRODUCT_CATEGORY
ORDER BY TOTAL_STOCK DESC;

-- üìà Chiffre d'affaires par jour (derniers 7 jours)
SELECT
    ORDER_DATE,
    COUNT(*) AS ORDER_COUNT,
    ROUND(SUM(TOTAL_AMOUNT), 2) AS DAILY_REVENUE
FROM PRODUCTION.ORDERS
WHERE ORDER_DATE >= DATEADD('day', -7, CURRENT_DATE())
GROUP BY ORDER_DATE
ORDER BY ORDER_DATE DESC;
```

---

## üîç Monitoring et D√©pannage

### **V√©rifier l'historique des tasks**

```sql
SELECT
    NAME AS TASK_NAME,
    STATE,
    SCHEDULED_TIME,
    COMPLETED_TIME,
    DATEDIFF('second', SCHEDULED_TIME, COMPLETED_TIME) AS DURATION_SECONDS,
    ERROR_CODE,
    ERROR_MESSAGE
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(
    SCHEDULED_TIME_RANGE_START => DATEADD('hour', -1, CURRENT_TIMESTAMP())
))
WHERE NAME LIKE 'TASK_%'
ORDER BY SCHEDULED_TIME DESC
LIMIT 20;
```

### **V√©rifier les streams**

```sql
-- Voir si des donn√©es sont en attente dans les streams
SELECT 'STREAM_RAW_EVENTS' AS STREAM_NAME, 
       SYSTEM$STREAM_HAS_DATA('STREAM_RAW_EVENTS') AS HAS_DATA,
       (SELECT COUNT(*) FROM STREAM_RAW_EVENTS) AS PENDING_ROWS
UNION ALL
SELECT 'STREAM_STG_ORDERS', 
       SYSTEM$STREAM_HAS_DATA('STREAM_STG_ORDERS'),
       (SELECT COUNT(*) FROM STREAM_STG_ORDERS);
```

### **V√©rifier les logs Docker**

```bash
# Consumer Kafka ‚Üí Snowflake
docker logs consumer --tail 100 --follow

# Producer (g√©n√®re les √©v√©nements)
docker logs producer --tail 50

# RedPanda (broker Kafka)
docker logs redpanda --tail 50
```

---

## ‚ö†Ô∏è Probl√®mes Courants et Solutions

### **‚ùå Probl√®me : Les tables STAGING/PRODUCTION sont vides**

**Causes possibles** :
1. Les tasks ne sont pas actives ‚Üí Ex√©cutez `SHOW TASKS;` et v√©rifiez `state = 'started'`
2. Les streams sont vides ‚Üí Ex√©cutez `SELECT COUNT(*) FROM STREAM_RAW_EVENTS;`
3. La condition `WHEN SYSTEM$STREAM_HAS_DATA(...)` retourne FALSE

**Solution rapide** :
```sql
-- Ex√©cuter manuellement les tasks
EXECUTE TASK TASK_RAW_TO_STAGING_DISTRIBUTOR;

-- Attendre 5 secondes
SELECT SYSTEM$WAIT(5, 'SECONDS');

-- V√©rifier STAGING
SELECT COUNT(*) FROM STAGING.STG_ORDERS;
```

---

### **‚ùå Probl√®me : Consumer Docker affiche des erreurs**

**Erreur** : `Table 'RAW_EVENTS_STREAM' does not exist`

**Solution** :
```bash
# Red√©marrer le consumer pour qu'il recr√©e la table
docker restart consumer

# Attendre 10 secondes
sleep 10

# V√©rifier les logs
docker logs consumer --tail 30
```

---

### **‚ùå Probl√®me : Warehouse suspendu**

**Erreur dans TASK_HISTORY** : `Warehouse 'COMPUTE_WH' not available`

**Solution** :
```sql
-- Activer le warehouse
ALTER WAREHOUSE COMPUTE_WH RESUME;

-- V√©rifier qu'il tourne
SHOW WAREHOUSES LIKE 'COMPUTE_WH';
```

---

## üìä M√©triques de Performance

### **D√©bit attendu** :

| Composant | M√©trique | Valeur |
|-----------|----------|--------|
| Producer | √âv√©nements/sec | ~100-200 |
| Consumer | Batch size | ~100-2500 √©v√©nements |
| Consumer | Latence | ~2-3 secondes/batch |
| Tasks Snowflake | Fr√©quence | Toutes les 1 minute |
| Tasks Snowflake | Dur√©e | ~2-5 secondes |

---

## üìÅ Fichiers du Projet

```
Projet-Snowflake/
‚îú‚îÄ‚îÄ docker-compose.yml                          # Infrastructure Kafka
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_producer.py                       # G√©n√®re les √©v√©nements
‚îÇ   ‚îî‚îÄ‚îÄ kafka_consumer_snowflake.py             # Ingestion Snowflake
‚îú‚îÄ‚îÄ snowflake-tasks-streams-CORRECTED.sql       # üéØ Script principal
‚îú‚îÄ‚îÄ fix-recreate-raw-table.sql                  # Utilitaire de r√©paration
‚îú‚îÄ‚îÄ execute-tasks-now.sql                       # Ex√©cution manuelle
‚îú‚îÄ‚îÄ SNOWFLAKE_AUTOMATION_GUIDE.md               # Documentation d√©taill√©e
‚îî‚îÄ‚îÄ README-DEPLOYMENT.md                        # üìñ Ce fichier
```

---

## ‚úÖ Checklist Finale

- [ ] Docker Desktop en cours d'ex√©cution
- [ ] Producer, Consumer, RedPanda actifs (`docker ps`)
- [ ] Consumer ins√®re dans Snowflake sans erreurs (`docker logs consumer`)
- [ ] RAW_EVENTS_STREAM contient des donn√©es (`SELECT COUNT(*)...`)
- [ ] Script `snowflake-tasks-streams-CORRECTED.sql` ex√©cut√© avec succ√®s
- [ ] 4 tasks actives (`SHOW TASKS;` ‚Üí state = 'started')
- [ ] STAGING et PRODUCTION remplis apr√®s 2-3 minutes
- [ ] Requ√™tes analytiques retournent des r√©sultats

---

## üéØ Prochaines √âtapes (Optionnel)

### **A. Cr√©er un Dashboard Snowflake**

Cr√©ez des vues mat√©rialis√©es pour des requ√™tes plus rapides :

```sql
CREATE OR REPLACE VIEW ANALYTICS.DAILY_SALES AS
SELECT
    ORDER_DATE,
    COUNT(*) AS ORDER_COUNT,
    SUM(TOTAL_AMOUNT) AS DAILY_REVENUE,
    AVG(TOTAL_AMOUNT) AS AVG_ORDER_VALUE
FROM PRODUCTION.ORDERS
GROUP BY ORDER_DATE;
```

### **B. Ajouter des alertes**

```sql
-- Alerte : Stock critique (< 10 unit√©s)
SELECT 
    PRODUCT_NAME,
    CURRENT_STOCK_LEVEL,
    WAREHOUSE_LOCATION
FROM PRODUCTION.INVENTORY_CURRENT
WHERE CURRENT_STOCK_LEVEL < 10
ORDER BY CURRENT_STOCK_LEVEL ASC;
```

### **C. Exporter vers Tableau/PowerBI**

Connectez Tableau Desktop √† Snowflake :
- Host : `<your-account>.snowflakecomputing.com`
- Database : `CAVES_ALBERT_DB`
- Schema : `PRODUCTION`
- Tables : `ORDERS`, `INVENTORY_CURRENT`

---

## üìû Support

Si vous rencontrez des probl√®mes :
1. V√©rifiez les logs : `docker logs consumer --tail 100`
2. V√©rifiez l'historique des tasks : `SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(...))`
3. Consultez `SNOWFLAKE_AUTOMATION_GUIDE.md` pour plus de d√©tails

---

**üç∑ Bon d√©ploiement avec Les Caves d'Albert ! üç∑**
