# ðŸš€ Streaming - Les Caves d'Albert

Ce dossier contient le **systÃ¨me de streaming en temps rÃ©el** pour l'ingestion d'Ã©vÃ©nements de vente de vin via Kafka â†’ Snowflake.

---

## ðŸ“‹ Contenu

### Fichiers Python

#### `kafka_producer.py`
**GÃ©nÃ©rateur d'Ã©vÃ©nements de vente en temps rÃ©el**

- **Objectif**: Produire des Ã©vÃ©nements de vente rÃ©alistes vers Kafka topic `sales_events`
- **Technologies**: Python, kafka-python
- **Types d'Ã©vÃ©nements**:
  - `ORDER_CREATED` (70%): Commandes clients avec dÃ©tails produit ðŸ›’
  - `INVENTORY_ADJUSTED` (30%): Ajustements de stock ðŸ“¦
- **Features**:
  - Emojis par catÃ©gorie (ðŸ· Rouge, ðŸ¥‚ Blanc, ðŸŒ¸ RosÃ©, ðŸ¾ Effervescent, ðŸ¥ƒ Spiritueux)
  - Noms de produits franÃ§ais (Merlot, Chardonnay, etc.)
  - Logique de pricing avec rÃ©ductions (25% chance)
  - Tracking d'entrepÃ´t pour ajustements
  - GÃ©nÃ©ration consistante via seeding (product_id-based)

**Configuration**:
```python
EVENT_GENERATION_DELAY = 1  # 1 Ã©vÃ©nement/seconde
ORDER_CREATED_PROBABILITY = 0.7  # 70% orders, 30% inventory
PRODUCT_ID_RANGE = (1000, 1049)  # 50 produits
CUSTOMER_ID_RANGE = (1, 150)  # 150 clients
```

**Exemple d'Ã©vÃ©nement ORDER_CREATED**:
```json
{
  "order_line_id": 67890,
  "customer_id": 42,
  "product_id": 1015,
  "product_name": "Chardonnay RÃ©serve ðŸ¥‚",
  "category": "Blanc",
  "quantity": 6,
  "unit_price": 18.50,
  "discount": 0.15,
  "total_price": 94.35,
  "bottle_size_liters": 0.75,
  "sales_channel": "E-com"
}
```

#### `kafka_consumer_snowflake.py`
**Consommateur Kafka â†’ Snowflake avec monitoring Prometheus**

- **Objectif**: IngÃ©rer les Ã©vÃ©nements Kafka vers Snowflake table `RAW_EVENTS_STREAM`
- **Architecture**: ELT (Event, Load, Transform)
- **Technologies**: Python, kafka-python, snowflake-sqlalchemy, prometheus-client
- **Features**:
  - 11 mÃ©triques Prometheus (counters, histograms, gauges)
  - Validation de schÃ©ma par type d'Ã©vÃ©nement
  - Dead Letter Queue (DLQ) pour Ã©vÃ©nements invalides
  - Batch processing (100 events/batch)
  - Monitoring de la table de staging
  - Graceful shutdown (SIGINT/SIGTERM)
  - Statistiques de session (running totals)

**MÃ©triques Prometheus** (exposÃ©es sur port 8000):
```
kafka_events_consumed_total         # Total events read
kafka_events_inserted_snowflake_total  # Total inserted
kafka_events_dlq_total              # Invalid events
kafka_batch_size                    # Batch distribution
kafka_batch_processing_duration_seconds  # Processing time
snowflake_insert_duration_seconds   # Snowflake insert time
kafka_current_batch_size            # Current batch gauge
kafka_snowflake_insert_rows         # Last insert count
kafka_consumer_lag                  # Consumer lag
kafka_event_size_bytes              # Event size distribution
```

#### `requirements-consumer.txt`
**DÃ©pendances Python**

```
kafka-python==2.0.2
sqlalchemy==1.4.46
snowflake-sqlalchemy==1.4.7
pandas==2.0.3
prometheus-client==0.17.1
python-dotenv==1.0.0
```

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kafka_producer.py  â”‚  GÃ©nÃ¨re Ã©vÃ©nements (ORDER_CREATED, INVENTORY_ADJUSTED)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ events/sec
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RedPanda Kafka    â”‚  Topic: sales_events (port 19092)
â”‚   (Docker)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ batch of 100
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚kafka_consumer_      â”‚  Validation â†’ Snowflake â†’ Metrics
â”‚snowflake.py         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Snowflake         â”‚    â”‚  Prometheus      â”‚
â”‚ RAW_EVENTS_STREAM   â”‚    â”‚  (port 9090)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    Grafana       â”‚
                           â”‚  (port 3000)     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Utilisation

### Avec Docker (RecommandÃ©)

**1. DÃ©marrer le stack complet**:
```bash
# Depuis la racine du projet
docker-compose up -d
```

Cela dÃ©marre:
- RedPanda (Kafka) sur port 19092
- RedPanda Console sur port 8080
- Producer (gÃ©nÃ¨re Ã©vÃ©nements)
- Consumer (ingÃ¨re vers Snowflake + expose mÃ©triques port 8000)

**2. DÃ©marrer le monitoring**:
```bash
docker-compose -f docker-compose-monitoring.yml up -d
```

Cela dÃ©marre:
- Prometheus sur port 9090
- Grafana sur port 3000 (admin/admin123)

**3. VÃ©rifier les logs**:
```bash
docker logs producer -f   # Voir Ã©vÃ©nements gÃ©nÃ©rÃ©s
docker logs consumer -f   # Voir ingestion Snowflake
```

---

### Localement (DÃ©veloppement)

**1. Installer les dÃ©pendances**:
```bash
cd streaming
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements-consumer.txt
```

**2. Configurer l'environnement**:

CrÃ©er `.env` Ã  la racine du projet:
```bash
# Kafka
KAFKA_TOPIC_NAME=sales_events

# Snowflake
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account.region
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=WINE_SALES_DB
SNOWFLAKE_SCHEMA=RAW_DATA
```

**3. DÃ©marrer RedPanda** (si pas Docker Compose):
```bash
docker run -d --name redpanda \
  -p 19092:19092 -p 9644:9644 \
  docker.redpanda.com/redpandadata/redpanda:latest \
  redpanda start --smp 1 --overprovisioned --node-id 0 \
  --kafka-addr INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:19092 \
  --advertise-kafka-addr INTERNAL://redpanda:9092,OUTSIDE://localhost:19092
```

**4. Lancer le producer**:
```bash
python kafka_producer.py
```

Logs:
```
ðŸš€ Kafka Producer started - Topic: sales_events
ðŸ›’ [ORDER_CREATED] Event 1 sent | Product: Merlot Tradition ðŸ· | Customer: 42
ðŸ“¦ [INVENTORY_ADJUSTED] Event 2 sent | Product: Chardonnay ðŸ¥‚ | Warehouse: Paris
```

**5. Lancer le consumer** (dans un autre terminal):
```bash
python kafka_consumer_snowflake.py
```

Logs:
```
ðŸš€ Starting Kafka Consumer for Snowflake ingestion
âœ… Connected to Snowflake: WINE_SALES_DB.RAW_DATA
ðŸ“Š Events consumed: 100 | Inserted: 98 | DLQ: 2 | Total: 100
```

**6. VÃ©rifier les mÃ©triques**:
```bash
curl http://localhost:8000/metrics
```

---

## ðŸ“Š SchÃ©ma Snowflake

```sql
-- Table crÃ©Ã©e automatiquement par le consumer
CREATE TABLE IF NOT EXISTS RAW_EVENTS_STREAM (
    EVENT_TYPE VARCHAR(50),        -- 'ORDER_CREATED', 'INVENTORY_ADJUSTED'
    PRODUCT_ID INTEGER,             -- Index pour analytics produit
    CUSTOMER_ID INTEGER,            -- Index pour analytics client
    EVENT_METADATA OBJECT,          -- MÃ©tadonnÃ©es (timestamp, source, etc.)
    EVENT_CONTENT VARIANT,          -- JSON complet de l'Ã©vÃ©nement
    INGESTION_TIME TIMESTAMP_LTZ    -- Timestamp d'ingestion automatique
);

-- Table de staging (temporaire, doit rester vide)
CREATE TABLE IF NOT EXISTS stg_raw_events_stream (
    EVENT_TYPE VARCHAR(50),
    PRODUCT_ID INTEGER,
    CUSTOMER_ID INTEGER,
    EVENT_METADATA OBJECT,
    EVENT_CONTENT VARIANT
);
```

### RequÃªtes d'analyse

```sql
-- Top 10 produits vendus
SELECT 
    PARSE_JSON(EVENT_CONTENT):product_name::STRING as product,
    COUNT(*) as orders,
    SUM(PARSE_JSON(EVENT_CONTENT):total_price::DECIMAL(10,2)) as revenue
FROM RAW_EVENTS_STREAM
WHERE EVENT_TYPE = 'ORDER_CREATED'
GROUP BY 1
ORDER BY revenue DESC
LIMIT 10;

-- Ventes par canal sur les derniÃ¨res 24h
SELECT 
    PARSE_JSON(EVENT_CONTENT):sales_channel::STRING as channel,
    COUNT(*) as orders,
    AVG(PARSE_JSON(EVENT_CONTENT):total_price::DECIMAL(10,2)) as avg_order
FROM RAW_EVENTS_STREAM
WHERE EVENT_TYPE = 'ORDER_CREATED'
  AND INGESTION_TIME >= DATEADD(hour, -24, CURRENT_TIMESTAMP())
GROUP BY 1;

-- Ajustements d'inventaire par type
SELECT 
    PARSE_JSON(EVENT_CONTENT):adjustment_type::STRING as type,
    COUNT(*) as adjustments,
    SUM(PARSE_JSON(EVENT_CONTENT):quantity_change::INTEGER) as total_quantity
FROM RAW_EVENTS_STREAM
WHERE EVENT_TYPE = 'INVENTORY_ADJUSTED'
GROUP BY 1;
```

---

## ðŸ“ˆ Monitoring Grafana

**Dashboard URL**: [http://localhost:3000/d/kafka-consumer-snowflake](http://localhost:3000/d/kafka-consumer-snowflake)

**8 Panels**:
1. **Events Consumed Rate** - Timeseries du dÃ©bit d'ingestion
2. **Total Events Consumed** - Compteur cumulÃ©
3. **Current Batch Size** - Taille du batch actuel
4. **Batch Processing Duration** - p50 et p95 percentiles
5. **Snowflake Insert Duration** - p50 et p95 percentiles
6. **Events Inserted to Snowflake** - Rate d'insertion
7. **DLQ Messages** - Ã‰vÃ©nements invalides (erreurs)
8. **Events Summary** - Table rÃ©capitulative par type

---

## ðŸ”„ DiffÃ©rence Batch vs Streaming

| Aspect | Streaming (ce dossier) | Batch (`/batch-ingestion`) |
|--------|------------------------|----------------------------|
| **Latence** | < 1 seconde | Minutes Ã  heures |
| **Volume** | Ã‰vÃ©nement par Ã©vÃ©nement | Bulk (milliers de lignes) |
| **Source** | Kafka topic `sales_events` | SQLite `wine_data.db` |
| **Destination** | `RAW_DATA.RAW_EVENTS_STREAM` | `BATCH_DATA.sales` |
| **Monitoring** | Prometheus + Grafana | Logs notebook |
| **Use Case** | Analytics temps rÃ©el | Chargement historique |

---

## ðŸ› ï¸ Troubleshooting

**ProblÃ¨me**: Producer ne peut pas se connecter Ã  Kafka
```
âœ… VÃ©rifier que RedPanda est dÃ©marrÃ©: docker ps | grep redpanda
âœ… Tester la connexion: telnet localhost 19092
```

**ProblÃ¨me**: Consumer erreur "Invalid credentials" Snowflake
```
âœ… VÃ©rifier .env avec les bons credentials
âœ… Tester: snowsql -a <account> -u <user>
```

**ProblÃ¨me**: Pas de mÃ©triques dans Grafana
```
âœ… VÃ©rifier consumer expose metrics: curl http://localhost:8000/metrics
âœ… VÃ©rifier Prometheus scrape: http://localhost:9090/targets
âœ… VÃ©rifier dashboard JSON est bien chargÃ©
```

**ProblÃ¨me**: Table stg_raw_events_stream contient des donnÃ©es
```
âœ… Le consumer log un WARNING au dÃ©marrage si non vide
âœ… C'est une table temporaire pour pandasâ†’Snowflake, doit s'auto-nettoyer
âœ… Si persiste: TRUNCATE TABLE stg_raw_events_stream;
```

---

## ðŸ“š Documentation ComplÃ¨te

- **[KAFKA_PRODUCER_IMPROVEMENTS.md](../KAFKA_PRODUCER_IMPROVEMENTS.md)**: DÃ©tails des amÃ©liorations producer
- **[KAFKA_CONSUMER_README.md](../KAFKA_CONSUMER_README.md)**: Documentation technique consumer (3500 mots)
- **[QUICK_START.md](../QUICK_START.md)**: Guide opÃ©rationnel complet
- **[CONSUMER_IMPROVEMENTS.md](../CONSUMER_IMPROVEMENTS.md)**: RÃ©sumÃ© des amÃ©liorations
- **[SAMPLE_OUTPUT.md](../SAMPLE_OUTPUT.md)**: Exemples de logs et JSON

---

**Pour l'ingestion batch historique, voir le dossier `/batch-ingestion` ðŸ“¦**
