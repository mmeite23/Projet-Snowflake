# üç∑ Les Caves d'Albert - Snowflake Tasks & Streams

## üìå Vue d'ensemble

Ce dossier contient l'impl√©mentation compl√®te d'un **pipeline automatis√© Snowflake** pour traiter les √©v√©nements en temps r√©el de la plateforme "Les Caves d'Albert" (boutique de vins en ligne).

### Architecture compl√®te

```
Kafka Producer ‚Üí RedPanda ‚Üí Consumer ‚Üí Snowflake RAW ‚Üí STAGING ‚Üí PRODUCTION
                                          ‚Üì         ‚Üì         ‚Üì
                                      Streams   Tasks    Analytics
```

---

## üìÇ Fichiers disponibles

| Fichier | Description | Usage |
|---------|-------------|-------|
| **`snowflake-tasks-streams.sql`** | Script principal de d√©ploiement | Create toutes les tables, streams et tasks |
| **`SNOWFLAKE_AUTOMATION_GUIDE.md`** | Guide complet d'impl√©mentation | Documentation technique d√©taill√©e |
| **`ARCHITECTURE_DIAGRAM.md`** | Diagrammes et flux de donn√©es | Comprendre l'architecture visuelle |
| **`analytical-queries.sql`** | 50+ requ√™tes pr√™tes √† l'emploi | Analyses business et dashboards |

---

## üöÄ Quick Start (5 minutes)

### Prerequisites

- ‚úÖ Base de donn√©es `CAVES_ALBERT_DB` cr√©√©e dans Snowflake
- ‚úÖ Sch√©mas `RAW_DATA`, `STAGING`, `PRODUCTION` cr√©√©s
- ‚úÖ Warehouse `COMPUTE_WH` disponible
- ‚úÖ Consumer Kafka ing√®re des donn√©es dans `RAW_EVENTS_STREAM`

### Installation en 3 √©tapes

#### 1Ô∏è‚É£ Verify que des donn√©es arrivent

```sql
USE DATABASE CAVES_ALBERT_DB;
USE SCHEMA RAW_DATA;

SELECT COUNT(*) FROM RAW_EVENTS_STREAM;
-- R√©sultat attendu : > 0 √©v√©nements
```

#### 2Ô∏è‚É£ Execute le script de d√©ploiement

```sql
-- Copier/coller le contenu de snowflake-tasks-streams.sql
-- dans Snowflake et execute tout le script
```

#### 3Ô∏è‚É£ Verify l'activation

```sql
-- Verify que les tasks sont actives
SHOW TASKS IN SCHEMA RAW_DATA;
-- R√©sultat : STATE = 'started' pour toutes les tasks

-- Attendre 2-3 minutes et verify les donn√©es
SELECT COUNT(*) FROM PRODUCTION.ORDERS;
SELECT COUNT(*) FROM PRODUCTION.INVENTORY_CURRENT;
```

‚úÖ **C'est tout !** Le pipeline est maintenant op√©rationnel.

---

## üìä Architecture des donn√©es

### ü•â Layer 1: RAW (Bronze)

**Table : `RAW_EVENTS_STREAM`**
- Donn√©es brutes ing√©r√©es depuis Kafka
- Format JSON dans `EVENT_DATA` (VARIANT)
- Conservation : 30 jours

### ü•à Layer 2: STAGING (Silver)

**Tables :**
- `STG_ORDERS` - Commandes transform√©es (JSON ‚Üí colonnes)
- `STG_INVENTORY_ADJUSTMENTS` - Ajustements d'inventaire transform√©s
- Conservation : 7 jours

### ü•á Layer 3: PRODUCTION (Gold)

**Tables :**
- `ORDERS` - Toutes les commandes clients (table finale)
- `INVENTORY_CURRENT` - √âtat actuel de l'inventaire (1 ligne/produit)
- `INVENTORY_HISTORY` - Historique complet des ajustements (audit trail)
- Conservation : Illimit√©e

---

## üîÑ Pipeline automatis√© (Tasks & Streams)

### DAG des Tasks

```
TASK_RAW_TO_STAGING_ORDERS (1 min)
    ‚îî‚îÄ‚Üí TASK_STAGING_TO_PROD_ORDERS (trigger after parent)

TASK_RAW_TO_STAGING_INVENTORY (1 min)
    ‚îî‚îÄ‚Üí TASK_STAGING_TO_PROD_INVENTORY_HISTORY (trigger after parent)
        ‚îî‚îÄ‚Üí TASK_STAGING_TO_PROD_INVENTORY_CURRENT (trigger after parent)
```

### Caract√©ristiques

- ‚ö° **Latence** : < 3 minutes de bout en bout
- üí∞ **Co√ªt optimis√©** : Tasks s'ex√©cutent uniquement si donn√©es pr√©sentes
- üîí **Fiabilit√©** : CDC natif avec Snowflake Streams
- üìä **Tra√ßabilit√©** : Timestamps √† chaque √©tape

---

## üìà Cas d'usage analytiques

Le fichier `analytical-queries.sql` contient **50+ requ√™tes** organis√©es en 10 sections :

### 1. üìä KPI temps r√©el
- Dashboard du jour (commandes, revenus, clients)
- Comparaison aujourd'hui vs hier
- √âtat de l'inventaire en temps r√©el

### 2. üèÜ Top performers
- Top 10 produits par revenus
- Top 10 produits par volume
- Top 20 clients par valeur
- Top cat√©gories de produits

### 3. üìà Tendances et √©volution
- √âvolution des ventes par jour/semaine/mois
- Analyse de saisonnalit√© (jour de la semaine, heures de pointe)
- Tendance par cat√©gorie

### 4. üîç Analyse des clients
- Segmentation RFM (Recency, Frequency, Monetary)
- Clients fid√®les (5+ commandes)
- Clients inactifs (30+ jours sans achat)

### 5. üì¶ Gestion d'inventaire
- Alertes de stock bas (< 50 unit√©s)
- Pr√©vision de rupture de stock
- Historique des ajustements
- Rotation des stocks (turnover rate)

### 6. üí∞ Analyse financi√®re
- Revenus par mode de paiement
- Panier moyen par cat√©gorie
- √âvolution mensuelle (MoM)

### 7. üéØ Performance produit
- Produits les plus rentables
- Analyse ABC (Pareto 80/20)
- Produits en d√©clin

### 8. üîî Alertes et monitoring
- Vue d'ensemble des alertes
- Rapport de sant√© du pipeline

### 9. üìä Vues mat√©rialis√©es
- `VW_DAILY_KPI` - KPI quotidiens
- `VW_PRODUCT_PERFORMANCE` - Performance par produit
- `VW_INVENTORY_ALERTS` - Inventaire avec alertes

### 10. üéì Exemples d'usage
- Dashboard manager
- Rapport hebdomadaire
- Liste de commandes √† pr√©parer

---

## üéØ Exemples concrets

### Exemple 1 : Top 5 vins les plus vendus cette semaine

```sql
SELECT
    PRODUCT_NAME,
    SUM(QUANTITY) AS UNITS_SOLD,
    SUM(TOTAL_AMOUNT) AS REVENUE
FROM PRODUCTION.ORDERS
WHERE ORDER_DATE >= DATE_TRUNC('week', CURRENT_DATE())
GROUP BY PRODUCT_NAME
ORDER BY UNITS_SOLD DESC
LIMIT 5;
```

### Exemple 2 : Clients VIP (> 1000‚Ç¨ de commandes)

```sql
SELECT
    CUSTOMER_ID,
    COUNT(*) AS ORDER_COUNT,
    SUM(TOTAL_AMOUNT) AS LIFETIME_VALUE
FROM PRODUCTION.ORDERS
GROUP BY CUSTOMER_ID
HAVING LIFETIME_VALUE > 1000
ORDER BY LIFETIME_VALUE DESC;
```

### Exemple 3 : Out of stock products de stock

```sql
SELECT
    PRODUCT_NAME,
    PRODUCT_CATEGORY,
    CURRENT_STOCK_LEVEL,
    WAREHOUSE_LOCATION
FROM PRODUCTION.INVENTORY_CURRENT
WHERE CURRENT_STOCK_LEVEL = 0
ORDER BY PRODUCT_CATEGORY;
```

---

## üìä Monitoring en temps r√©el

### Verify l'√©tat du pipeline

```sql
-- 1. √âtat des tasks
SHOW TASKS IN SCHEMA RAW_DATA;

-- 2. Historique des ex√©cutions (derni√®res 10)
SELECT
    NAME,
    STATE,
    SCHEDULED_TIME,
    COMPLETED_TIME,
    DATEDIFF('second', SCHEDULED_TIME, COMPLETED_TIME) AS DURATION_SEC
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE NAME LIKE 'TASK_%'
ORDER BY SCHEDULED_TIME DESC
LIMIT 10;

-- 3. Verify les streams (donn√©es en attente)
SELECT 'STREAM_RAW_ORDERS' AS STREAM_NAME, 
       SYSTEM$STREAM_HAS_DATA('STREAM_RAW_ORDERS') AS HAS_DATA
UNION ALL
SELECT 'STREAM_RAW_INVENTORY', 
       SYSTEM$STREAM_HAS_DATA('STREAM_RAW_INVENTORY');

-- 4. Pipeline latency
SELECT
    MAX(INGESTION_TIMESTAMP) AS LAST_RAW_INGESTION,
    MAX(UPDATED_AT) AS LAST_PROD_UPDATE,
    DATEDIFF('minute', MAX(UPDATED_AT), CURRENT_TIMESTAMP()) AS LATENCY_MINUTES
FROM PRODUCTION.ORDERS;
```

### Dashboard de sant√©

```sql
-- Vue d'ensemble du nombre de lignes par table
SELECT 'RAW_EVENTS_STREAM' AS TABLE_NAME, COUNT(*) AS ROW_COUNT FROM RAW_DATA.RAW_EVENTS_STREAM
UNION ALL
SELECT 'STG_ORDERS', COUNT(*) FROM STAGING.STG_ORDERS
UNION ALL
SELECT 'STG_INVENTORY_ADJ', COUNT(*) FROM STAGING.STG_INVENTORY_ADJUSTMENTS
UNION ALL
SELECT 'ORDERS', COUNT(*) FROM PRODUCTION.ORDERS
UNION ALL
SELECT 'INVENTORY_CURRENT', COUNT(*) FROM PRODUCTION.INVENTORY_CURRENT
UNION ALL
SELECT 'INVENTORY_HISTORY', COUNT(*) FROM PRODUCTION.INVENTORY_HISTORY;
```

---

## üõ†Ô∏è Commandes de gestion

### Suspendre temporairement les tasks

```sql
-- Suspendre dans l'ordre inverse (enfants d'abord)
ALTER TASK TASK_STAGING_TO_PROD_INVENTORY_CURRENT SUSPEND;
ALTER TASK TASK_STAGING_TO_PROD_INVENTORY_HISTORY SUSPEND;
ALTER TASK TASK_STAGING_TO_PROD_ORDERS SUSPEND;
ALTER TASK TASK_RAW_TO_STAGING_INVENTORY SUSPEND;
ALTER TASK TASK_RAW_TO_STAGING_ORDERS SUSPEND;
```

### R√©activate les tasks

```sql
-- Activate dans l'ordre normal (parents d'abord)
ALTER TASK TASK_RAW_TO_STAGING_ORDERS RESUME;
ALTER TASK TASK_RAW_TO_STAGING_INVENTORY RESUME;
ALTER TASK TASK_STAGING_TO_PROD_ORDERS RESUME;
ALTER TASK TASK_STAGING_TO_PROD_INVENTORY_HISTORY RESUME;
ALTER TASK TASK_STAGING_TO_PROD_INVENTORY_CURRENT RESUME;
```

### Execute manuellement une task (test)

```sql
EXECUTE TASK TASK_RAW_TO_STAGING_ORDERS;
```

### Clean les old data

```sql
-- Delete les donn√©es RAW de more than 30 jours
DELETE FROM RAW_DATA.RAW_EVENTS_STREAM
WHERE INGESTION_TIMESTAMP < DATEADD('day', -30, CURRENT_TIMESTAMP());

-- Delete les donn√©es STAGING de more than 7 jours
DELETE FROM STAGING.STG_ORDERS
WHERE INGESTION_TIMESTAMP < DATEADD('day', -7, CURRENT_TIMESTAMP());
```

---

## üîß Troubleshooting

### Probl√®me : Les tasks ne s'ex√©cutent pas

```sql
-- 1. Verify l'√©tat des tasks
SHOW TASKS;

-- 2. Voir les erreurs
SELECT NAME, ERROR_CODE, ERROR_MESSAGE, QUERY_TEXT
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE STATE = 'FAILED'
ORDER BY SCHEDULED_TIME DESC
LIMIT 5;

-- 3. R√©activate si n√©cessaire
ALTER TASK TASK_RAW_TO_STAGING_ORDERS RESUME;
```

### Probl√®me : Les streams sont vides

```sql
-- Verify que des donn√©es arrivent dans RAW
SELECT COUNT(*) FROM RAW_DATA.RAW_EVENTS_STREAM
WHERE INGESTION_TIMESTAMP > DATEADD('minute', -5, CURRENT_TIMESTAMP());

-- Si 0 : Verify que le consumer Kafka fonctionne
-- docker logs consumer-caves-albert
```

### Probl√®me : Latence excessive (> 5 min)

```sql
-- Verify la dur√©e d'ex√©cution des tasks
SELECT
    NAME,
    DATEDIFF('second', SCHEDULED_TIME, COMPLETED_TIME) AS DURATION_SEC
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE NAME LIKE 'TASK_%'
    AND SCHEDULED_TIME > DATEADD('hour', -1, CURRENT_TIMESTAMP())
ORDER BY DURATION_SEC DESC;

-- Si trop long : Augmenter la taille du warehouse
-- ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'MEDIUM';
```

---

## üí∞ Optimisation des co√ªts

### Strat√©gies

1. **Tasks conditionnelles** : Les tasks ne s'ex√©cutent que si `SYSTEM$STREAM_HAS_DATA() = TRUE`
2. **Auto-suspend warehouse** : Le warehouse se suspend automatiquement entre les ex√©cutions
3. **Nettoyage r√©gulier** : Delete les old data RAW et STAGING
4. **Clustering keys** : Add des cl√©s de clustering sur les grandes tables

### Exemple de clustering

```sql
-- Optimiser les requ√™tes par date
ALTER TABLE PRODUCTION.ORDERS 
CLUSTER BY (ORDER_DATE);

-- Optimiser les requ√™tes par produit
ALTER TABLE PRODUCTION.INVENTORY_CURRENT 
CLUSTER BY (PRODUCT_CATEGORY, PRODUCT_ID);
```

---

## üìö Documentation

- **`SNOWFLAKE_AUTOMATION_GUIDE.md`** : Guide complet avec explications d√©taill√©es
- **`ARCHITECTURE_DIAGRAM.md`** : Diagrammes visuels du flux de donn√©es
- **`analytical-queries.sql`** : Toutes les requ√™tes pr√™tes √† l'emploi

### Resources externes

- [Snowflake Streams Documentation](https://docs.snowflake.com/en/user-guide/streams-intro)
- [Snowflake Tasks Documentation](https://docs.snowflake.com/en/user-guide/tasks-intro)
- [CDC with Streams](https://docs.snowflake.com/en/user-guide/streams-cdc)

---

## üéØ Avantages de cette architecture

### ‚úÖ Temps r√©el
- Latence < 3 minutes de bout en bout
- Donn√©es disponibles pour l'analytique quasi instantan√©ment
- Support de millions d'√©v√©nements par jour

### ‚úÖ Fiabilit√©
- CDC natif avec Snowflake Streams
- Garantie exactly-once processing
- Tra√ßabilit√© compl√®te avec timestamps

### ‚úÖ Scalabilit√©
- Architecture modulaire (RAW ‚Üí STAGING ‚Üí PROD)
- Facile d'add de nouveaux types d'√©v√©nements
- Support de la croissance du volume de donn√©es

### ‚úÖ Maintenabilit√©
- SQL pur, pas de code externe
- Monitoring int√©gr√©
- Facile √† d√©bugger et √† modify

### ‚úÖ Co√ªt optimis√©
- Tasks conditionnelles (uniquement si donn√©es pr√©sentes)
- Warehouse auto-suspend
- Pas de cr√©dits Snowflake gaspill√©s

---

## üöÄ Prochaines √©tapes

### Am√©liorations recommand√©es

1. **Alerting** : Configurer des alertes Snowflake pour les anomalies
2. **DBT** : Int√©grer DBT pour la gestion des transformations
3. **Great Expectations** : Add des tests de qualit√© de donn√©es
4. **Tableau/Power BI** : Connecter un outil BI pour des dashboards visuels
5. **Machine Learning** : Pr√©visions de demande avec Snowflake ML

### Int√©grations possibles

- **Salesforce** : Synchroniser les donn√©es clients
- **Shopify** : Int√©grer les commandes e-commerce
- **Google Analytics** : Enrichir avec les donn√©es web
- **SendGrid** : Automatiser les emails marketing

---

## üë• Support

Pour toute question ou assistance :

- üìß Email : data-team@lescavesdalbert.com
- üìö Documentation : Lire `SNOWFLAKE_AUTOMATION_GUIDE.md`
- üêõ Issues : Ouvrir un ticket sur GitHub

---

## üìÑ Licence

MIT License - Libre d'utilisation et de modification

---

**üç∑ Bon traitement de donn√©es avec Les Caves d'Albert !**

*Cr√©√© avec ‚ù§Ô∏è pour d√©montrer les meilleures pratiques Snowflake en production*
