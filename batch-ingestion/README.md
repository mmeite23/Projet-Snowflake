# üì¶ Batch Ingestion - Les Caves d'Albert

Ce dossier contient les **notebooks et scripts d'ingestion en batch** pour le projet Les Caves d'Albert.

---

## üìã Contenu

### Notebooks Jupyter

#### `Data_generator_faker.ipynb`
**G√©n√©rateur de donn√©es de vente de vin avec Faker**

- **Objectif**: G√©n√©rer des donn√©es de vente r√©alistes pour tests et d√©monstrations
- **Sortie**: Base de donn√©es SQLite (`wine_data.db`) avec tables `customers`, `inventory`, `sales`
- **Technologies**: Python, Faker, SQLite, Pandas
- **Donn√©es g√©n√©r√©es**:
  - 50 produits (vins et spiritueux) avec cat√©gories, prix, tailles de bouteilles
  - 100+ clients avec noms fran√ßais (Faker locale `fr_FR`)
  - Transactions de vente sur 10 jours avec logique de stock
- **Seed**: 11 (reproductibilit√© garantie)

**Structure des donn√©es**:
```sql
-- Table customers
customer_id, name, email, registration_date

-- Table inventory
product_id, product_name, category, price_per_unit, bottle_size_liters, stock_quantity

-- Table sales
sale_id, sale_date, customer_id, product_id, quantity_sold, total_price, sales_channel
```

#### `Pipeline.ipynb`
**Pipeline d'ingestion SQLite ‚Üí Snowflake**

- **Objectif**: Charger les donn√©es batch depuis SQLite vers Snowflake
- **Architecture**: ELT (Extract, Load, Transform)
- **√âtapes**:
  1. **Extract**: Lecture des tables SQLite (`customers`, `inventory`, `sales`)
  2. **Load**: Insertion dans Snowflake via `pandas.to_sql()` + SQLAlchemy
  3. **Transform**: Transformations SQL dans Snowflake (optionnel)
- **Technologies**: Python, SQLite, Snowflake, SQLAlchemy, Pandas
- **S√©curit√©**: Utilise `.env` pour credentials Snowflake (via `python-dotenv`)

**Configuration requise** (`.env`):
```bash
# Source SQLite
DB_PATH=test_data/wine_data.db

# Target Snowflake
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account.region
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=WINE_SALES_DB
SNOWFLAKE_SCHEMA=BATCH_DATA
```

---

### Autres fichiers

#### `Git_Basics.ipynb`
**Tutoriel Git pour d√©butants**

- Guide d'installation et configuration Git
- Commandes de base (init, add, commit, push, pull)
- Workflow Git standard
- Configuration VSCode comme √©diteur par d√©faut

#### `Screenshot Snwoflake.png`
**Capture d'√©cran Snowflake**

- Screenshot historique de l'interface Snowflake
- Utile pour documentation visuelle

---

## üîÑ Diff√©rence Batch vs Streaming

| Aspect | Batch Ingestion (ce dossier) | Streaming (`/streaming`) |
|--------|------------------------------|--------------------------|
| **Fr√©quence** | Manuel ou schedul√© (quotidien, hebdomadaire) | En temps r√©el (continu) |
| **Volume** | Gros volumes (historique) | √âv√©nements individuels |
| **Latence** | Minutes √† heures | Millisecondes √† secondes |
| **Cas d'usage** | Chargement initial, rapports mensuels | Analytics temps r√©el, alertes |
| **Technologies** | SQLite, Pandas, Notebooks | Kafka, RedPanda, Producer/Consumer |
| **Source** | `wine_data.db` (SQLite) | Kafka topic `sales_events` |
| **Destination** | Snowflake schema `BATCH_DATA` | Snowflake schema `RAW_DATA` |

---

## üöÄ Utilisation

### 1. G√©n√©rer les donn√©es (Data_generator_faker.ipynb)

```bash
# Dans VSCode, ouvrir le notebook et ex√©cuter toutes les cellules
# Ou en ligne de commande:
jupyter notebook Data_generator_faker.ipynb
```

Cela cr√©era `test_data/wine_data.db` avec ~100 clients, 50 produits, et transactions sur 10 jours.

### 2. Charger vers Snowflake (Pipeline.ipynb)

**Pr√©requis**:
- Fichier `.env` configur√© avec credentials Snowflake
- Base de donn√©es `wine_data.db` g√©n√©r√©e

```bash
jupyter notebook Pipeline.ipynb
```

Le notebook va:
1. Se connecter √† SQLite (`wine_data.db`)
2. Extraire les 3 tables
3. Se connecter √† Snowflake
4. Cr√©er les tables si elles n'existent pas
5. Ins√©rer les donn√©es

---

## üìä Sch√©ma Snowflake (Batch)

```sql
-- Database: WINE_SALES_DB
-- Schema: BATCH_DATA

-- Table customers (dimension)
CREATE TABLE customers (
    customer_id INTEGER PRIMARY KEY,
    name VARCHAR(200),
    email VARCHAR(200),
    registration_date DATE
);

-- Table inventory (dimension)
CREATE TABLE inventory (
    product_id INTEGER PRIMARY KEY,
    product_name VARCHAR(200),
    category VARCHAR(50),
    price_per_unit DECIMAL(10,2),
    bottle_size_liters DECIMAL(5,3),
    stock_quantity INTEGER
);

-- Table sales (fait)
CREATE TABLE sales (
    sale_id INTEGER PRIMARY KEY,
    sale_date TIMESTAMP,
    customer_id INTEGER,
    product_id INTEGER,
    quantity_sold INTEGER,
    total_price DECIMAL(10,2),
    sales_channel VARCHAR(100)
);
```

---

## üîó Relation avec le Streaming

Les donn√©es batch servent de **baseline historique**, tandis que le streaming (`/streaming`) capture les **√©v√©nements temps r√©el**.

**Exemple de workflow combin√©**:
1. **Batch** (ce dossier): Charger 1 an d'historique de ventes via `Pipeline.ipynb`
2. **Streaming** (`/streaming`): √Ä partir d'aujourd'hui, ing√©rer les nouvelles ventes en temps r√©el via Kafka

**Consolidation dans Snowflake**:
```sql
-- Vue combin√©e batch + streaming
CREATE VIEW consolidated_sales AS
SELECT 
    sale_date,
    customer_id,
    product_id,
    quantity_sold,
    total_price,
    'BATCH' as source
FROM BATCH_DATA.sales

UNION ALL

SELECT 
    PARSE_JSON(event_content):timestamp::TIMESTAMP as sale_date,
    PARSE_JSON(event_content):customer_id::INTEGER as customer_id,
    PARSE_JSON(event_content):product_id::INTEGER as product_id,
    PARSE_JSON(event_content):quantity::INTEGER as quantity_sold,
    PARSE_JSON(event_content):total_price::DECIMAL(10,2) as total_price,
    'STREAMING' as source
FROM RAW_DATA.raw_events_stream
WHERE event_type = 'ORDER_CREATED';
```

---

## üìö D√©pendances

```bash
# Pour Data_generator_faker.ipynb
pip install faker pandas

# Pour Pipeline.ipynb
pip install pandas sqlalchemy snowflake-sqlalchemy python-dotenv

# Pour ex√©cuter les notebooks
pip install jupyter notebook
```

---

## üõ†Ô∏è Troubleshooting

**Probl√®me**: `wine_data.db` n'existe pas
- ‚úÖ Ex√©cuter d'abord `Data_generator_faker.ipynb` pour g√©n√©rer la base

**Probl√®me**: Erreur de connexion Snowflake dans `Pipeline.ipynb`
- ‚úÖ V√©rifier le fichier `.env` avec les bons credentials
- ‚úÖ Tester la connexion : `snowsql -a <account> -u <user>`

**Probl√®me**: `ModuleNotFoundError: No module named 'faker'`
- ‚úÖ Installer les d√©pendances : `pip install faker pandas jupyter`

---

**Pour le streaming temps r√©el, voir le dossier `/streaming` üöÄ**
