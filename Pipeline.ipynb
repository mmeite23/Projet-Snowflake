{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edd2226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Path: /Users/mory_jr/Library/DBeaverData/workspace6/.metadata/sample-database-sqlite-1/Chinook.db\n",
      "Configuration loaded securely and ready for use.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Configuration (Secure Loading)\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "load_dotenv() # Load variables from .env file\n",
    "\n",
    "# A. Source Info\n",
    "DB_PATH = os.getenv('DB_PATH')\n",
    "print(f\"Database Path: {DB_PATH}\")\n",
    "# B. Snowflake Credentials (Retrieve all required variables)\n",
    "# ... (Retrieve all SNOWFLAKE_* variables) ...\n",
    "SNOWFLAKE_USER = os.getenv('SNOWFLAKE_USER')\n",
    "SNOWFLAKE_PASSWORD = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "SNOWFLAKE_ACCOUNT = os.getenv('SNOWFLAKE_ACCOUNT')\n",
    "SNOWFLAKE_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE')\n",
    "SNOWFLAKE_DATABASE = os.getenv('SNOWFLAKE_DATABASE')\n",
    "TARGET_SCHEMA = os.getenv('SNOWFLAKE_SCHEMA') \n",
    "\n",
    "print(\"Configuration loaded securely and ready for use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eda54bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite established at: /Users/mory_jr/Library/DBeaverData/workspace6/.metadata/sample-database-sqlite-1/Chinook.db\n",
      "SQLite connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline.ipynb - Cell 2: Data Extraction from Local SQLite\n",
    "\n",
    "if not DB_PATH:\n",
    "    raise ValueError(\"Error: 'DB_PATH' environment variable is not loaded. Please run Cell 1.\")\n",
    "\n",
    "TABLE_QUERIES = {\n",
    "    'customers_df': \"SELECT * FROM customers;\",\n",
    "    'inventory_df': \"SELECT * FROM inventory;\",\n",
    "    'sales_df': \"SELECT * FROM sales;\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "conn = None\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(f\"Connection to SQLite established at: {DB_PATH}\")\n",
    "\n",
    "    for df_name, query in TABLE_QUERIES.items():\n",
    "        dataframes[df_name] = pd.read_sql_query(query, conn)\n",
    "\n",
    "    # Assign DataFrames for easy access\n",
    "    customers_df = dataframes['customers_df']\n",
    "    inventory_df = dataframes['inventory_df']\n",
    "    sales_df = dataframes['sales_df']\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"FATAL: SQLite error during data extraction: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print(\"SQLite connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ebed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory: 3 rows removed due to quality issues.\n",
      "Data joined. Final sales lines: 943\n",
      "Transformation complÃ¨te. DataFrame 'final_fact_sales_df' est prÃªt pour le chargement (MERGE).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. DATA CLEANING AND VALIDATION (Inventory) ---\n",
    "# Filter out intentional errors (negative stock, invalid price/size)\n",
    "inventory_df_clean = inventory_df[inventory_df['stock_quantity'] >= 0].copy()\n",
    "inventory_df_clean = inventory_df_clean[inventory_df_clean['unit_price'] > 0]\n",
    "inventory_df_clean = inventory_df_clean[inventory_df_clean['bottle_size_l'].isin([0.375, 0.5, 0.75, 1.0, 1.5])]\n",
    "print(f\"Inventory: {len(inventory_df) - len(inventory_df_clean)} rows removed due to quality issues.\")\n",
    "\n",
    "\n",
    "# --- 2. JOINING TABLES (Ensuring Referential Integrity) ---\n",
    "# A. Join Sales with CLEAN Inventory\n",
    "# Suffixes pour renommer 'unit_price' en 'unit_price_sale' (de sales_df) et 'unit_price_base' (de inventory_df_clean)\n",
    "sales_inventory_join = sales_df.merge(\n",
    "    inventory_df_clean[['product_id', 'product_name', 'category', 'unit_price']],\n",
    "    on='product_id',\n",
    "    how='inner',\n",
    "    suffixes=('_sale', '_base')\n",
    ")\n",
    "\n",
    "# B. Join with Customers\n",
    "fact_sales = sales_inventory_join.merge(\n",
    "    customers_df[['customer_id', 'city', 'channel']],\n",
    "    on='customer_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Data joined. Final sales lines: {len(fact_sales)}\")\n",
    "\n",
    "\n",
    "# --- 3. FEATURE ENGINEERING & METRIC CALCULATION ---\n",
    "\n",
    "# Calculer les mÃ©triques en utilisant la colonne 'unit_price_sale' sÃ©curisÃ©e\n",
    "fact_sales['total_price'] = fact_sales['unit_price_sale'] * fact_sales['quantity']\n",
    "fact_sales['discount_amount'] = fact_sales['total_price'] * (fact_sales['discount'] / 100)\n",
    "fact_sales['net_revenue'] = fact_sales['total_price'] - fact_sales['discount_amount']\n",
    "\n",
    "# Calcul de la marge brute\n",
    "fact_sales['cost_of_goods'] = fact_sales['unit_price_base'] * fact_sales['quantity']\n",
    "fact_sales['gross_margin'] = fact_sales['net_revenue'] - fact_sales['cost_of_goods']\n",
    "\n",
    "# Transformation des colonnes de date et temps\n",
    "fact_sales['TXN_DATE'] = pd.to_datetime(fact_sales['sold_at']).dt.date\n",
    "fact_sales['TXN_TIMESTAMP'] = pd.to_datetime(fact_sales['sold_at'])\n",
    "\n",
    "\n",
    "# --- 4. FINAL DATA SELECTION AND RENAMING (CORRECTED) ---\n",
    "# Utiliser 'order_id' Ã  la place de 'order_line_id'\n",
    "# Retirer 'sales_channel_sale' (qui est la version suffixÃ©e de sales_channel), car 'sales_channel' n'existe pas dans le sales_df initial\n",
    "final_fact_sales_df = fact_sales[[\n",
    "    'order_id',          # CORRECTION : Utiliser order_id (la colonne existante)\n",
    "    'TXN_DATE', \n",
    "    'TXN_TIMESTAMP', \n",
    "    'product_id', \n",
    "    'customer_id',      \n",
    "    'net_revenue', \n",
    "    'gross_margin', \n",
    "    'city', \n",
    "    'category', \n",
    "    'channel'            # Le canal d'acquisition du client est conservÃ©\n",
    "    # 'SALES_CHANNEL' n'est plus sÃ©lectionnÃ© ici car il manque dans sales_df\n",
    "]].rename(columns={\n",
    "    'order_id': 'ORDER_ID',          # Renommage de la clÃ© pour la FACT\n",
    "    'product_id': 'PRODUCT_ID',\n",
    "    'customer_id': 'CUSTOMER_ID',\n",
    "    'net_revenue': 'NET_REVENUE',\n",
    "    'gross_margin': 'GROSS_MARGIN',\n",
    "    'city': 'CUSTOMER_CITY',\n",
    "    'category': 'PRODUCT_CATEGORY',\n",
    "    'channel': 'ACQUISITION_CHANNEL', \n",
    "    # 'SALES_CHANNEL': 'SALES_CHANNEL' (supprimÃ©)\n",
    "})\n",
    "\n",
    "print(\"Transformation complÃ¨te. DataFrame 'final_fact_sales_df' est prÃªt pour le chargement (MERGE).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e7f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'product_id', 'customer_id', 'quantity', 'unit_price',\n",
      "       'discount', 'sold_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# EXÃ‰CUTER SEULEMENT POUR DEBUG\n",
    "print(sales_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db82165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›‘ ATTENTION: Doublons trouvÃ©s dans le DataFrame avant le chargement !\n",
      "-----------------------------------------------------------------\n",
      "     ORDER_ID    TXN_DATE       TXN_TIMESTAMP  PRODUCT_ID  CUSTOMER_ID  \\\n",
      "1           2  2025-09-01 2025-09-01 05:45:12        1030           19   \n",
      "568         2  2025-09-07 2025-09-07 19:37:20        1030           77   \n",
      "3           4  2025-09-01 2025-09-01 15:31:34        1004           98   \n",
      "570         4  2025-09-07 2025-09-07 10:10:32        1004           59   \n",
      "376         5  2025-09-05 2025-09-05 15:04:49        1033           87   \n",
      "..        ...         ...                 ...         ...          ...   \n",
      "836        97  2025-09-09 2025-09-09 19:03:02        1004           39   \n",
      "270        98  2025-09-03 2025-09-03 06:30:00        1004          102   \n",
      "936        98  2025-09-10 2025-09-10 19:35:17        1004           41   \n",
      "369       105  2025-09-04 2025-09-04 06:10:19        1043           74   \n",
      "561       105  2025-09-06 2025-09-06 13:32:59        1043           33   \n",
      "\n",
      "     NET_REVENUE  GROSS_MARGIN        CUSTOMER_CITY PRODUCT_CATEGORY  \\\n",
      "1      43.901676     -0.768324      Masse-les-Bains     Effervescent   \n",
      "568   130.445334     -3.564666          MichelVille     Effervescent   \n",
      "3      52.400000      0.000000             Gonzalez     Effervescent   \n",
      "570   104.800000      0.000000   Saint TristanBourg     Effervescent   \n",
      "376    27.830000      0.000000              Rousset     Effervescent   \n",
      "..           ...           ...                  ...              ...   \n",
      "836    52.400000      0.000000    Sainte Ã‰liseboeuf     Effervescent   \n",
      "270   157.200000      0.000000            Hoaraunec     Effervescent   \n",
      "936    52.400000      0.000000                Rossi     Effervescent   \n",
      "369    88.340000      0.000000  Normand-sur-Humbert       Spiritueux   \n",
      "561    44.170000      0.000000                Merle       Spiritueux   \n",
      "\n",
      "    ACQUISITION_CHANNEL  \n",
      "1              en ligne  \n",
      "568            boutique  \n",
      "3              boutique  \n",
      "570            en ligne  \n",
      "376            boutique  \n",
      "..                  ...  \n",
      "836            en ligne  \n",
      "270            boutique  \n",
      "936            en ligne  \n",
      "369            boutique  \n",
      "561            boutique  \n",
      "\n",
      "[146 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# --- DEBUG: VÃ©rifier les doublons avant le chargement ---\n",
    "\n",
    "# Identifier les lignes oÃ¹ la combinaison de ORDER_ID et PRODUCT_ID est dupliquÃ©e\n",
    "duplicate_keys = final_fact_sales_df.duplicated(subset=['ORDER_ID', 'PRODUCT_ID'], keep=False)\n",
    "\n",
    "# Afficher toutes les lignes qui partagent une clÃ© (ORDER_ID, PRODUCT_ID)\n",
    "if duplicate_keys.any():\n",
    "    print(\"ðŸ›‘ ATTENTION: Doublons trouvÃ©s dans le DataFrame avant le chargement !\")\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    # Trier pour voir les doublons cÃ´te Ã  cÃ´te\n",
    "    print(final_fact_sales_df[duplicate_keys].sort_values(by=['ORDER_ID', 'PRODUCT_ID']))\n",
    "else:\n",
    "    print(\"âœ… Aucune clÃ© (ORDER_ID, PRODUCT_ID) dupliquÃ©e n'a Ã©tÃ© trouvÃ©e. Le problÃ¨me est ailleurs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be97e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake Engine created.\n",
      "Taille initiale du DataFrame: 943 lignes.\n",
      "Taille du DataFrame aprÃ¨s dÃ©doublonnage: 867 lignes.\n",
      "Using schema: SALES_SCHEMA\n",
      "\n",
      "1. Preparing STAGING table: SALES_SCHEMA.STG_FACT_SALES\n",
      "   -> Dropped existing staging table (if any).\n",
      "   -> Created new empty staging table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/1714805748.py:65: UserWarning: The provided table name 'STG_FACT_SALES' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  final_fact_sales_df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Loaded 867 rows into staging table.\n",
      "\n",
      "2. Executing MERGE from staging to final table: SALES_SCHEMA.FACT_SALES\n",
      "   -> MERGE executed successfully.\n",
      "\n",
      "3. Staging table STG_FACT_SALES dropped.\n",
      "\n",
      "4. Loading Dimension Tables (Strategy: Drop and Create)\n",
      "   -> Preparing to load DIM_CUSTOMERS. Dropping if it exists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/1714805748.py:115: UserWarning: The provided table name 'DIM_CUSTOMERS' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Table DIM_CUSTOMERS loaded successfully.\n",
      "   -> Preparing to load DIM_INVENTORY. Dropping if it exists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/1714805748.py:115: UserWarning: The provided table name 'DIM_INVENTORY' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Table DIM_INVENTORY loaded successfully.\n",
      "\n",
      "ðŸŽ‰ ETL Batch Pipeline (avec MERGE) ComplÃ©tÃ© avec SuccÃ¨s !\n"
     ]
    }
   ],
   "source": [
    "# Pipeline.ipynb - Cell 4: Data Loading to Snowflake using MERGE Strategy (CORRIGÃ‰E ET FIABILISÃ‰E)\n",
    "\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "if final_fact_sales_df.empty:\n",
    "    print(\"ðŸ›‘ Loading aborted: The final_fact_sales_df is empty.\")\n",
    "else:\n",
    "    # --- 1. BUILD CONNECTION AND ENGINE ---\n",
    "    try:\n",
    "        SNOWFLAKE_URL = URL(\n",
    "            account=SNOWFLAKE_ACCOUNT,\n",
    "            user=SNOWFLAKE_USER,\n",
    "            password=SNOWFLAKE_PASSWORD,\n",
    "            database=SNOWFLAKE_DATABASE,\n",
    "            warehouse=SNOWFLAKE_WAREHOUSE\n",
    "        )\n",
    "        snowflake_engine = create_engine(SNOWFLAKE_URL)\n",
    "        print(\"Snowflake Engine created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ›‘ FATAL: Failed to create Snowflake Engine: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. SETUP DATA AND TARGETS ---\n",
    "    FACT_TABLE_NAME = 'FACT_SALES'\n",
    "    STAGING_TABLE_NAME = 'STG_FACT_SALES'\n",
    "    \n",
    "    # S'assurer que les colonnes du DataFrame sont en majuscules\n",
    "    final_fact_sales_df.columns = [col.upper() for col in final_fact_sales_df.columns]\n",
    "    \n",
    "    # ðŸŽ¯ CORRECTION FINALE : DÃ‰DOUBLONNAGE AVANT CHARGEMENT\n",
    "    # On garantit l'unicitÃ© de la clÃ© (ORDER_ID, PRODUCT_ID) en ne gardant que la derniÃ¨re entrÃ©e\n",
    "    # basÃ©e sur le timestamp de la transaction.\n",
    "    print(f\"Taille initiale du DataFrame: {len(final_fact_sales_df)} lignes.\")\n",
    "    final_fact_sales_df.sort_values('TXN_TIMESTAMP', ascending=True, inplace=True)\n",
    "    final_fact_sales_df.drop_duplicates(subset=['ORDER_ID', 'PRODUCT_ID'], keep='last', inplace=True)\n",
    "    print(f\"Taille du DataFrame aprÃ¨s dÃ©doublonnage: {len(final_fact_sales_df)} lignes.\")\n",
    "\n",
    "    dataframes_to_replace = {\n",
    "        'DIM_CUSTOMERS': customers_df,       \n",
    "        'DIM_INVENTORY': inventory_df_clean \n",
    "    }\n",
    "\n",
    "    # --- 3. MERGE LOGIC EXECUTION ---\n",
    "    with snowflake_engine.connect() as connection:\n",
    "        transaction = None \n",
    "        try:\n",
    "            transaction = connection.begin() \n",
    "            \n",
    "            # A. PRÃ‰PARATION du SchÃ©ma\n",
    "            connection.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SNOWFLAKE_DATABASE}.{TARGET_SCHEMA};\"))\n",
    "            connection.execute(text(f\"USE SCHEMA {SNOWFLAKE_DATABASE}.{TARGET_SCHEMA};\"))\n",
    "            print(f\"Using schema: {TARGET_SCHEMA}\")\n",
    "\n",
    "            # B. STRATÃ‰GIE MERGE pour FACT_SALES\n",
    "            print(f\"\\n1. Preparing STAGING table: {TARGET_SCHEMA}.{STAGING_TABLE_NAME}\")\n",
    "            \n",
    "            connection.execute(text(f\"DROP TABLE IF EXISTS {TARGET_SCHEMA}.{STAGING_TABLE_NAME};\"))\n",
    "            print(f\"   -> Dropped existing staging table (if any).\")\n",
    "\n",
    "            create_staging_table_sql = psql.get_schema(final_fact_sales_df, STAGING_TABLE_NAME, con=connection)\n",
    "            connection.execute(text(create_staging_table_sql))\n",
    "            print(f\"   -> Created new empty staging table.\")\n",
    "\n",
    "            final_fact_sales_df.to_sql(\n",
    "                name=STAGING_TABLE_NAME,\n",
    "                con=connection,\n",
    "                schema=TARGET_SCHEMA,      \n",
    "                if_exists='append', \n",
    "                index=False,               \n",
    "                chunksize=16000            \n",
    "            )\n",
    "            print(f\"   -> Loaded {len(final_fact_sales_df)} rows into staging table.\")\n",
    "\n",
    "            # Ã‰TAPE 2: EXÃ‰CUTION du MERGE (La requÃªte est dÃ©jÃ  correcte)\n",
    "            print(f\"\\n2. Executing MERGE from staging to final table: {TARGET_SCHEMA}.{FACT_TABLE_NAME}\")\n",
    "            \n",
    "            connection.execute(text(f\"CREATE TABLE IF NOT EXISTS {TARGET_SCHEMA}.{FACT_TABLE_NAME} LIKE {TARGET_SCHEMA}.{STAGING_TABLE_NAME};\"))\n",
    "\n",
    "            merge_query = f\"\"\"\n",
    "            MERGE INTO {TARGET_SCHEMA}.{FACT_TABLE_NAME} AS target\n",
    "            USING {TARGET_SCHEMA}.{STAGING_TABLE_NAME} AS staging\n",
    "            ON target.ORDER_ID = staging.ORDER_ID AND target.PRODUCT_ID = staging.PRODUCT_ID\n",
    "            \n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET \n",
    "                    target.NET_REVENUE = staging.NET_REVENUE,\n",
    "                    target.GROSS_MARGIN = staging.GROSS_MARGIN,\n",
    "                    target.TXN_TIMESTAMP = staging.TXN_TIMESTAMP \n",
    "            \n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT (ORDER_ID, TXN_DATE, TXN_TIMESTAMP, PRODUCT_ID, CUSTOMER_ID, NET_REVENUE, \n",
    "                        GROSS_MARGIN, CUSTOMER_CITY, PRODUCT_CATEGORY, ACQUISITION_CHANNEL)\n",
    "                VALUES (staging.ORDER_ID, staging.TXN_DATE, staging.TXN_TIMESTAMP, staging.PRODUCT_ID, \n",
    "                        staging.CUSTOMER_ID, staging.NET_REVENUE, staging.GROSS_MARGIN, staging.CUSTOMER_CITY, \n",
    "                        staging.PRODUCT_CATEGORY, staging.ACQUISITION_CHANNEL);\n",
    "            \"\"\"\n",
    "            \n",
    "            connection.execute(text(merge_query))\n",
    "            print(\"   -> MERGE executed successfully.\")\n",
    "\n",
    "            # Le reste du code...\n",
    "            connection.execute(text(f\"DROP TABLE {TARGET_SCHEMA}.{STAGING_TABLE_NAME};\"))\n",
    "            print(f\"\\n3. Staging table {STAGING_TABLE_NAME} dropped.\")\n",
    "\n",
    "            print(\"\\n4. Loading Dimension Tables (Strategy: Drop and Create)\")\n",
    "            for table_name, df in dataframes_to_replace.items():\n",
    "                df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "                # Ã‰TAPE 1: Supprimer manuellement la table pour Ã©viter les erreurs de \"reflection\"\n",
    "                print(f\"   -> Preparing to load {table_name}. Dropping if it exists...\")\n",
    "                connection.execute(text(f\"DROP TABLE IF EXISTS {TARGET_SCHEMA}.{table_name};\"))\n",
    "\n",
    "                # Ã‰TAPE 2: Charger les donnÃ©es. Pandas va maintenant crÃ©er la table sans erreur.\n",
    "                df.to_sql(\n",
    "                    name=table_name,\n",
    "                    con=connection,\n",
    "                    schema=TARGET_SCHEMA,      \n",
    "                    if_exists='fail', # Comportement par dÃ©faut : Ã©choue si la table existe dÃ©jÃ .\n",
    "                                      # C'est parfait car nous venons de la supprimer.\n",
    "                    index=False,               \n",
    "                    chunksize=16000            \n",
    "                )\n",
    "                print(f\"   âœ… Table {table_name} loaded successfully.\")\n",
    "\n",
    "            \n",
    "            transaction.commit()\n",
    "            print(\"\\nðŸŽ‰ ETL Batch Pipeline (avec MERGE) ComplÃ©tÃ© avec SuccÃ¨s !\")\n",
    "\n",
    "        except SQLAlchemyError as e:\n",
    "            print(f\"\\nðŸ›‘ FATAL: SQLAlchemy/Snowflake error during loading: {e}\")\n",
    "            if transaction:\n",
    "                transaction.rollback()\n",
    "                print(\"   -> Transaction has been rolled back.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nðŸ›‘ FATAL: An unexpected error occurred: {e}\")\n",
    "            if transaction:\n",
    "                transaction.rollback()\n",
    "                print(\"   -> Transaction has been rolled back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a670e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake Engine created successfully.\n",
      "\n",
      "Ensuring target schema 'SALES_SCHEMA' exists in database 'ETL_PROJECT_DB'...\n",
      "Schema check/creation complete.\n",
      "Loading DataFrame into Snowflake table: SALES_SCHEMA.FACT_SALES (867 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/830007204.py:63: UserWarning: The provided table name 'FACT_SALES' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table SALES_SCHEMA.FACT_SALES loaded successfully.\n",
      "Loading DataFrame into Snowflake table: SALES_SCHEMA.DIM_CUSTOMERS (121 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/830007204.py:63: UserWarning: The provided table name 'DIM_CUSTOMERS' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table SALES_SCHEMA.DIM_CUSTOMERS loaded successfully.\n",
      "Loading DataFrame into Snowflake table: SALES_SCHEMA.DIM_INVENTORY (47 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sq/ygf5k1rd0qj2p7yhjhf884f80000gn/T/ipykernel_53332/830007204.py:63: UserWarning: The provided table name 'DIM_INVENTORY' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  df.to_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table SALES_SCHEMA.DIM_INVENTORY loaded successfully.\n",
      "\n",
      "ðŸŽ‰ ETL Batch Pipeline Completed Successfully! Data is live on Snowflake.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline.ipynb - Cell 5: Data Loading to Snowflake\n",
    "\n",
    "# Imports (Assuming create_engine, URL, OperationalError, text, os are from Cell 1)\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# --- 1. RETRIEVE CONFIGURATION ---\n",
    "# The target schema name (e.g., SALES_SCHEMA) should be set in your .env file\n",
    "# and retrieved in Cell 1 as SNOWFLAKE_SCHEMA\n",
    "SNOWFLAKE_USER = os.getenv('SNOWFLAKE_USER')\n",
    "SNOWFLAKE_PASSWORD = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "SNOWFLAKE_ACCOUNT = os.getenv('SNOWFLAKE_ACCOUNT')\n",
    "SNOWFLAKE_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE')\n",
    "SNOWFLAKE_DATABASE = os.getenv('SNOWFLAKE_DATABASE')\n",
    "TARGET_SCHEMA = os.getenv('SNOWFLAKE_SCHEMA') \n",
    "\n",
    "# --- 2. BUILD THE CONNECTION URL ---\n",
    "try:\n",
    "    # Use the URL object from snowflake.sqlalchemy for a robust connection string\n",
    "    SNOWFLAKE_URL = URL(\n",
    "        account=SNOWFLAKE_ACCOUNT,\n",
    "        user=SNOWFLAKE_USER,\n",
    "        password=SNOWFLAKE_PASSWORD,\n",
    "        database=SNOWFLAKE_DATABASE,\n",
    "        warehouse=SNOWFLAKE_WAREHOUSE\n",
    "    )\n",
    "\n",
    "    # --- 3. CREATE THE SQLALCHEMY ENGINE ---\n",
    "    snowflake_engine = create_engine(SNOWFLAKE_URL)\n",
    "    \n",
    "    print(\"Snowflake Engine created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nðŸ›‘ FATAL: Failed to create Snowflake Engine. Check credentials and network.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. DATA LOADING PROCESS ---\n",
    "if not final_fact_sales_df.empty:\n",
    "    \n",
    "    # Define DataFrames and their target table names\n",
    "    dataframes_to_load = {\n",
    "        'FACT_SALES': final_fact_sales_df,\n",
    "        'DIM_CUSTOMERS': customers_df,       # Load the source customers as a dimension\n",
    "        'DIM_INVENTORY': inventory_df_clean  # Load the cleaned inventory as a dimension\n",
    "    }\n",
    "    \n",
    "    with snowflake_engine.connect() as connection:\n",
    "        try:\n",
    "            # A. SCHEMA CREATION (DDL)\n",
    "            print(f\"\\nEnsuring target schema '{TARGET_SCHEMA}' exists in database '{SNOWFLAKE_DATABASE}'...\")\n",
    "            # Execute DDL to create schema if it doesn't exist.\n",
    "            connection.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {SNOWFLAKE_DATABASE}.{TARGET_SCHEMA};\"))\n",
    "            print(\"Schema check/creation complete.\")\n",
    "\n",
    "            # B. Loop through all DataFrames and load them\n",
    "            for table_name, df in dataframes_to_load.items():\n",
    "                print(f\"Loading DataFrame into Snowflake table: {TARGET_SCHEMA}.{table_name} ({len(df)} rows)\")\n",
    "\n",
    "                # Ensure columns are uppercase (best practice for Snowflake)\n",
    "                df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "                # Use to_sql() to perform the bulk insert\n",
    "                df.to_sql(\n",
    "                    table_name,\n",
    "                    con=connection,\n",
    "                    schema=TARGET_SCHEMA,      # Targets the dedicated schema\n",
    "                    if_exists='append',       # Append to the table (Batch append)\n",
    "                    index=False,               # Do not include the Pandas index\n",
    "                    chunksize=16000            # Optimize chunk size for bulk loading\n",
    "                )\n",
    "                print(f\"âœ… Table {TARGET_SCHEMA}.{table_name} loaded successfully.\")\n",
    "            \n",
    "            connection.commit() # Commit all table creations and loads\n",
    "            print(\"\\nðŸŽ‰ ETL Batch Pipeline Completed Successfully! Data is live on Snowflake.\")\n",
    "\n",
    "        except SQLAlchemyError as e:\n",
    "            # Rollback transaction on any SQL error\n",
    "            connection.rollback()\n",
    "            print(f\"\\nðŸ›‘ FATAL: SQLAlchemy/Snowflake error during loading: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nðŸ›‘ FATAL: An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"ðŸ›‘ Loading aborted: The final_fact_sales_df is empty.\")\n",
    "\n",
    "# The engine dispose happens implicitly when the process finishes or when the notebook stops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
